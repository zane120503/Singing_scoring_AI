#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved Smart Voice Detector - Ph√°t hi·ªán ch√≠nh x√°c v·ªã tr√≠ b·∫Øt ƒë·∫ßu gi·ªçng h√°t
"""

import os
import sys
import numpy as np
import librosa
import logging
from typing import Dict, List, Tuple, Optional
from pathlib import Path

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

logger = logging.getLogger(__name__)

class ImprovedSmartVoiceDetector:
    """Improved Smart Voice Detector - Ph√°t hi·ªán ch√≠nh x√°c v·ªã tr√≠ b·∫Øt ƒë·∫ßu gi·ªçng h√°t"""
    
    def __init__(self, sr: int = 22050):
        self.sr = sr
        self.frame_length = 2048
        self.hop_length = 512
    
    def detect_voice_activity(self, audio_path: str) -> List[Dict]:
        """Ph√°t hi·ªán voice activity v·ªõi ƒë·ªô ch√≠nh x√°c cao"""
        try:
            logger.info("üß† Improved Smart Voice Detection...")
            
            # Load audio
            audio, sr = librosa.load(audio_path, sr=self.sr)
            
            # Convert to mono if stereo
            if len(audio.shape) > 1:
                audio = librosa.to_mono(audio)
            
            # B∆∞·ªõc 1: Ph√¢n t√≠ch baseline (ƒëo·∫°n ƒë·∫ßu file)
            baseline_features = self._analyze_baseline(audio, sr)
            
            # B∆∞·ªõc 2: Ph√¢n t√≠ch energy pattern v·ªõi baseline c·∫£i thi·ªán
            energy_segments = self._detect_energy_with_improved_baseline(audio, sr, baseline_features)
            
            # B∆∞·ªõc 3: Ph√¢n t√≠ch spectral pattern v·ªõi baseline c·∫£i thi·ªán
            spectral_segments = self._detect_spectral_with_improved_baseline(audio, sr, baseline_features)
            
            # B∆∞·ªõc 4: Ph√¢n t√≠ch harmonic pattern c·∫£i thi·ªán
            harmonic_segments = self._detect_harmonic_pattern_improved(audio, sr)
            
            # B∆∞·ªõc 5: Ph√¢n t√≠ch voice characteristics c·∫£i thi·ªán
            voice_segments = self._detect_voice_characteristics_improved(audio, sr)
            
            # B∆∞·ªõc 6: K·∫øt h·ª£p v√† l·ªçc k·∫øt qu·∫£ th√¥ng minh
            combined_segments = self._smart_combine_segments_improved(
                energy_segments, spectral_segments, harmonic_segments, voice_segments, baseline_features
            )
            
            logger.info(f"‚úÖ Improved Smart voice detection: {len(combined_segments)} segments")
            return combined_segments
            
        except Exception as e:
            logger.error(f"‚ùå Improved Smart voice detection failed: {e}")
            return []
    
    def _analyze_baseline(self, audio: np.ndarray, sr: int) -> Dict:
        """Ph√¢n t√≠ch baseline c·ªßa file (ƒëo·∫°n ƒë·∫ßu) - c·∫£i thi·ªán"""
        try:
            # Ph√¢n t√≠ch 3 gi√¢y ƒë·∫ßu (tƒÉng t·ª´ 2s)
            baseline_length = min(3 * sr, len(audio))
            baseline_audio = audio[:baseline_length]
            
            # T√≠nh to√°n features c·ªßa baseline
            baseline_rms = librosa.feature.rms(y=baseline_audio)[0]
            baseline_centroids = librosa.feature.spectral_centroid(y=baseline_audio, sr=sr)[0]
            baseline_rolloff = librosa.feature.spectral_rolloff(y=baseline_audio, sr=sr)[0]
            baseline_zcr = librosa.feature.zero_crossing_rate(baseline_audio)[0]
            
            baseline_features = {
                'rms_mean': np.mean(baseline_rms),
                'rms_std': np.std(baseline_rms),
                'centroid_mean': np.mean(baseline_centroids),
                'centroid_std': np.std(baseline_centroids),
                'rolloff_mean': np.mean(baseline_rolloff),
                'rolloff_std': np.std(baseline_rolloff),
                'zcr_mean': np.mean(baseline_zcr),
                'zcr_std': np.std(baseline_zcr)
            }
            
            logger.info(f"üìä Improved Baseline analysis:")
            logger.info(f"   RMS: {baseline_features['rms_mean']:.4f} ¬± {baseline_features['rms_std']:.4f}")
            logger.info(f"   Centroid: {baseline_features['centroid_mean']:.1f} ¬± {baseline_features['centroid_std']:.1f} Hz")
            logger.info(f"   Rolloff: {baseline_features['rolloff_mean']:.1f} ¬± {baseline_features['rolloff_std']:.1f} Hz")
            logger.info(f"   ZCR: {baseline_features['zcr_mean']:.4f} ¬± {baseline_features['zcr_std']:.4f}")
            
            return baseline_features
            
        except Exception as e:
            logger.warning(f"Baseline analysis failed: {e}")
            return {
                'rms_mean': 0.05, 'rms_std': 0.02,
                'centroid_mean': 1000, 'centroid_std': 200,
                'rolloff_mean': 1200, 'rolloff_std': 300,
                'zcr_mean': 0.08, 'zcr_std': 0.02
            }
    
    def _detect_energy_with_improved_baseline(self, audio: np.ndarray, sr: int, baseline: Dict) -> List[Dict]:
        """Ph√°t hi·ªán energy pattern v·ªõi baseline c·∫£i thi·ªán"""
        try:
            # T√≠nh RMS energy
            rms = librosa.feature.rms(y=audio, frame_length=self.frame_length, hop_length=self.hop_length)[0]
            
            # Adaptive threshold d·ª±a tr√™n baseline - c·∫£i thi·ªán
            rms_threshold = baseline['rms_mean'] + baseline['rms_std'] * 0.3  # Gi·∫£m threshold ƒë·ªÉ ph√°t hi·ªán voice nh·∫π
            
            # T√¨m frames c√≥ energy cao
            voice_frames = []
            for i, energy in enumerate(rms):
                if energy > rms_threshold:
                    voice_frames.append(i)
            
            # Chuy·ªÉn frames th√†nh segments
            segments = self._frames_to_segments(voice_frames, sr)
            
            logger.info(f"üîã Energy detection: {len(segments)} segments, threshold: {rms_threshold:.4f}")
            return segments
            
        except Exception as e:
            logger.warning(f"Energy detection failed: {e}")
            return []
    
    def _detect_spectral_with_improved_baseline(self, audio: np.ndarray, sr: int, baseline: Dict) -> List[Dict]:
        """Ph√°t hi·ªán spectral pattern v·ªõi baseline c·∫£i thi·ªán"""
        try:
            # T√≠nh spectral features
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=self.hop_length)[0]
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, hop_length=self.hop_length)[0]
            
            # Adaptive thresholds d·ª±a tr√™n baseline - c·∫£i thi·ªán
            centroid_threshold = baseline['centroid_mean'] - baseline['centroid_std'] * 0.2  # Gi·∫£m threshold
            rolloff_threshold = baseline['rolloff_mean'] - baseline['rolloff_std'] * 0.2  # Gi·∫£m threshold
            
            # T√¨m frames c√≥ spectral features ph√π h·ª£p v·ªõi voice
            voice_frames = []
            for i in range(len(spectral_centroids)):
                if (spectral_centroids[i] > centroid_threshold and 
                    spectral_rolloff[i] > rolloff_threshold):
                    voice_frames.append(i)
            
            # Chuy·ªÉn frames th√†nh segments
            segments = self._frames_to_segments(voice_frames, sr)
            
            logger.info(f"üìä Spectral detection: {len(segments)} segments")
            return segments
            
        except Exception as e:
            logger.warning(f"Spectral detection failed: {e}")
            return []
    
    def _detect_harmonic_pattern_improved(self, audio: np.ndarray, sr: int) -> List[Dict]:
        """Ph√°t hi·ªán harmonic pattern c·∫£i thi·ªán"""
        try:
            # Harmonic-percussive separation
            harmonic, percussive = librosa.effects.hpss(audio)
            
            # T√≠nh harmonic energy
            harmonic_rms = librosa.feature.rms(y=harmonic, frame_length=self.frame_length, hop_length=self.hop_length)[0]
            
            # Threshold cho harmonic energy
            harmonic_threshold = np.percentile(harmonic_rms, 25)  # Gi·∫£m threshold
            
            # T√¨m frames c√≥ harmonic energy cao
            voice_frames = []
            for i, energy in enumerate(harmonic_rms):
                if energy > harmonic_threshold:
                    voice_frames.append(i)
            
            # Chuy·ªÉn frames th√†nh segments
            segments = self._frames_to_segments(voice_frames, sr)
            
            logger.info(f"üéµ Harmonic detection: {len(segments)} segments")
            return segments
            
        except Exception as e:
            logger.warning(f"Harmonic detection failed: {e}")
            return []
    
    def _detect_voice_characteristics_improved(self, audio: np.ndarray, sr: int) -> List[Dict]:
        """Ph√°t hi·ªán voice characteristics c·∫£i thi·ªán"""
        try:
            # MFCC features
            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, hop_length=self.hop_length)
            
            # Spectral contrast
            spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr, hop_length=self.hop_length)
            
            # Zero crossing rate
            zcr = librosa.feature.zero_crossing_rate(audio, frame_length=self.frame_length, hop_length=self.hop_length)[0]
            
            # Adaptive thresholds
            mfcc_threshold = np.percentile(mfccs[0], 20)  # Gi·∫£m threshold
            contrast_threshold = np.percentile(spectral_contrast[0], 20)  # Gi·∫£m threshold
            zcr_threshold = np.percentile(zcr, 20)  # Gi·∫£m threshold
            
            # T√¨m frames c√≥ voice characteristics
            voice_frames = []
            for i in range(len(mfccs[0])):
                if (mfccs[0][i] > mfcc_threshold and 
                    spectral_contrast[0][i] > contrast_threshold and
                    zcr[i] > zcr_threshold):
                    voice_frames.append(i)
            
            # Chuy·ªÉn frames th√†nh segments
            segments = self._frames_to_segments(voice_frames, sr)
            
            logger.info(f"üé§ Voice characteristics detection: {len(segments)} segments")
            return segments
            
        except Exception as e:
            logger.warning(f"Voice characteristics detection failed: {e}")
            return []
    
    def _smart_combine_segments_improved(self, energy_segments: List[Dict], spectral_segments: List[Dict], 
                                       harmonic_segments: List[Dict], voice_segments: List[Dict], 
                                       baseline: Dict) -> List[Dict]:
        """K·∫øt h·ª£p segments th√¥ng minh - c·∫£i thi·ªán"""
        try:
            # K·∫øt h·ª£p t·∫•t c·∫£ segments
            all_segments = energy_segments + spectral_segments + harmonic_segments + voice_segments
            
            # Merge overlapping segments
            merged_segments = self._merge_overlapping_segments(all_segments)
            
            # L·ªçc segments d·ª±a tr√™n baseline - c·∫£i thi·ªán
            filtered_segments = []
            for segment in merged_segments:
                # Ki·ªÉm tra duration
                duration = segment['end'] - segment['start']
                if duration < 0.5:  # B·ªè qua segments qu√° ng·∫Øn
                    continue
                
                # Ki·ªÉm tra v·ªã tr√≠ - b·ªè qua segments ·ªü ƒë·∫ßu file qu√° s·ªõm
                if segment['start'] < 1.0:  # B·ªè qua segments ·ªü ƒë·∫ßu file
                    continue
                
                # Ki·ªÉm tra confidence
                if segment['confidence'] < 0.3:  # Gi·∫£m threshold confidence
                    continue
                
                filtered_segments.append(segment)
            
            # S·∫Øp x·∫øp theo th·ªùi gian
            filtered_segments.sort(key=lambda x: x['start'])
            
            logger.info(f"üîó Smart combination: {len(filtered_segments)} segments after filtering")
            return filtered_segments
            
        except Exception as e:
            logger.warning(f"Smart combination failed: {e}")
            return []
    
    def _frames_to_segments(self, voice_frames: List[int], sr: int) -> List[Dict]:
        """Chuy·ªÉn frames th√†nh segments"""
        if not voice_frames:
            return []
        
        segments = []
        current_start_frame = voice_frames[0]
        current_end_frame = voice_frames[0]
        
        for i in range(1, len(voice_frames)):
            if voice_frames[i] == current_end_frame + 1:
                current_end_frame = voice_frames[i]
            else:
                segments.append({
                    'start': current_start_frame * self.hop_length / sr,
                    'end': (current_end_frame + 1) * self.hop_length / sr,
                    'confidence': 1.0,
                    'method': 'improved_smart_detection'
                })
                current_start_frame = voice_frames[i]
                current_end_frame = voice_frames[i]
        
        segments.append({
            'start': current_start_frame * self.hop_length / sr,
            'end': (current_end_frame + 1) * self.hop_length / sr,
            'confidence': 1.0,
            'method': 'improved_smart_detection'
        })
        
        return segments
    
    def _merge_overlapping_segments(self, segments: List[Dict]) -> List[Dict]:
        """Merge overlapping segments"""
        if not segments:
            return []
        
        # S·∫Øp x·∫øp theo start time
        segments.sort(key=lambda x: x['start'])
        
        merged = []
        current_segment = segments[0]
        
        for i in range(1, len(segments)):
            next_segment = segments[i]
            
            # N·∫øu segments overlap ho·∫∑c g·∫ßn nhau
            if next_segment['start'] - current_segment['end'] <= 0.5:
                current_segment['end'] = max(current_segment['end'], next_segment['end'])
                current_segment['confidence'] = max(current_segment['confidence'], next_segment['confidence'])
            else:
                merged.append(current_segment)
                current_segment = next_segment
        
        merged.append(current_segment)
        return merged
