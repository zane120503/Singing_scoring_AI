#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Improved Smart Voice Detector - PhÃ¡t hiá»‡n chÃ­nh xÃ¡c vá»‹ trÃ­ báº¯t Ä‘áº§u giá»ng hÃ¡t
"""

import os
import sys
import numpy as np
import librosa
import logging
from typing import Dict, List, Tuple, Optional
from pathlib import Path

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..', 'src'))

logger = logging.getLogger(__name__)

class ImprovedSmartVoiceDetector:
    """Improved Smart Voice Detector - PhÃ¡t hiá»‡n chÃ­nh xÃ¡c vá»‹ trÃ­ báº¯t Ä‘áº§u giá»ng hÃ¡t"""
    
    def __init__(self, sr: int = 22050):
        self.sr = sr
        self.frame_length = 2048
        self.hop_length = 512
    
    def detect_voice_activity(self, audio_path: str) -> List[Dict]:
        """PhÃ¡t hiá»‡n voice activity vá»›i Ä‘á»™ chÃ­nh xÃ¡c cao"""
        try:
            logger.info("ğŸ§  Improved Smart Voice Detection...")
            
            # Load audio
            audio, sr = librosa.load(audio_path, sr=self.sr)
            
            # Convert to mono if stereo
            if len(audio.shape) > 1:
                audio = librosa.to_mono(audio)
            
            # BÆ°á»›c 1: PhÃ¢n tÃ­ch baseline (Ä‘oáº¡n Ä‘áº§u file)
            baseline_features = self._analyze_baseline(audio, sr)
            
            # BÆ°á»›c 2: PhÃ¢n tÃ­ch energy pattern vá»›i baseline cáº£i thiá»‡n
            energy_segments = self._detect_energy_with_improved_baseline(audio, sr, baseline_features)
            
            # BÆ°á»›c 3: PhÃ¢n tÃ­ch spectral pattern vá»›i baseline cáº£i thiá»‡n
            spectral_segments = self._detect_spectral_with_improved_baseline(audio, sr, baseline_features)
            
            # BÆ°á»›c 4: PhÃ¢n tÃ­ch harmonic pattern cáº£i thiá»‡n
            harmonic_segments = self._detect_harmonic_pattern_improved(audio, sr)
            
            # BÆ°á»›c 5: PhÃ¢n tÃ­ch voice characteristics cáº£i thiá»‡n
            voice_segments = self._detect_voice_characteristics_improved(audio, sr)
            
            # BÆ°á»›c 6: Káº¿t há»£p vÃ  lá»c káº¿t quáº£ thÃ´ng minh
            combined_segments = self._smart_combine_segments_improved(
                energy_segments, spectral_segments, harmonic_segments, voice_segments, baseline_features
            )
            
            logger.info(f"âœ… Improved Smart voice detection: {len(combined_segments)} segments")
            return combined_segments
            
        except Exception as e:
            logger.error(f"âŒ Improved Smart voice detection failed: {e}")
            return []
    
    def _analyze_baseline(self, audio: np.ndarray, sr: int) -> Dict:
        """PhÃ¢n tÃ­ch baseline cá»§a file (Ä‘oáº¡n Ä‘áº§u) - cáº£i thiá»‡n"""
        try:
            # PhÃ¢n tÃ­ch 3 giÃ¢y Ä‘áº§u (tÄƒng tá»« 2s)
            baseline_length = min(3 * sr, len(audio))
            baseline_audio = audio[:baseline_length]
            
            # TÃ­nh toÃ¡n features cá»§a baseline
            baseline_rms = librosa.feature.rms(y=baseline_audio)[0]
            baseline_centroids = librosa.feature.spectral_centroid(y=baseline_audio, sr=sr)[0]
            baseline_rolloff = librosa.feature.spectral_rolloff(y=baseline_audio, sr=sr)[0]
            baseline_zcr = librosa.feature.zero_crossing_rate(baseline_audio)[0]
            
            baseline_features = {
                'rms_mean': np.mean(baseline_rms),
                'rms_std': np.std(baseline_rms),
                'centroid_mean': np.mean(baseline_centroids),
                'centroid_std': np.std(baseline_centroids),
                'rolloff_mean': np.mean(baseline_rolloff),
                'rolloff_std': np.std(baseline_rolloff),
                'zcr_mean': np.mean(baseline_zcr),
                'zcr_std': np.std(baseline_zcr)
            }
            
            logger.info(f"ğŸ“Š Improved Baseline analysis:")
            logger.info(f"   RMS: {baseline_features['rms_mean']:.4f} Â± {baseline_features['rms_std']:.4f}")
            logger.info(f"   Centroid: {baseline_features['centroid_mean']:.1f} Â± {baseline_features['centroid_std']:.1f} Hz")
            logger.info(f"   Rolloff: {baseline_features['rolloff_mean']:.1f} Â± {baseline_features['rolloff_std']:.1f} Hz")
            logger.info(f"   ZCR: {baseline_features['zcr_mean']:.4f} Â± {baseline_features['zcr_std']:.4f}")
            
            return baseline_features
            
        except Exception as e:
            logger.warning(f"Baseline analysis failed: {e}")
            return {
                'rms_mean': 0.05, 'rms_std': 0.02,
                'centroid_mean': 1000, 'centroid_std': 200,
                'rolloff_mean': 1200, 'rolloff_std': 300,
                'zcr_mean': 0.08, 'zcr_std': 0.02
            }
    
    def _detect_energy_with_improved_baseline(self, audio: np.ndarray, sr: int, baseline: Dict) -> List[Dict]:
        """PhÃ¡t hiá»‡n energy pattern vá»›i baseline cáº£i thiá»‡n"""
        try:
            # TÃ­nh RMS energy
            rms = librosa.feature.rms(y=audio, frame_length=self.frame_length, hop_length=self.hop_length)[0]
            
            # Adaptive threshold dá»±a trÃªn baseline - cáº£i thiá»‡n
            rms_threshold = baseline['rms_mean'] + baseline['rms_std'] * 0.3  # Giáº£m threshold Ä‘á»ƒ phÃ¡t hiá»‡n voice nháº¹
            
            # TÃ¬m frames cÃ³ energy cao
            voice_frames = []
            for i, energy in enumerate(rms):
                if energy > rms_threshold:
                    voice_frames.append(i)
            
            # Chuyá»ƒn frames thÃ nh segments
            segments = self._frames_to_segments(voice_frames, sr)
            
            logger.info(f"ğŸ”‹ Energy detection: {len(segments)} segments, threshold: {rms_threshold:.4f}")
            return segments
            
        except Exception as e:
            logger.warning(f"Energy detection failed: {e}")
            return []
    
    def _detect_spectral_with_improved_baseline(self, audio: np.ndarray, sr: int, baseline: Dict) -> List[Dict]:
        """PhÃ¡t hiá»‡n spectral pattern vá»›i baseline cáº£i thiá»‡n"""
        try:
            # TÃ­nh spectral features
            spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr, hop_length=self.hop_length)[0]
            spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr, hop_length=self.hop_length)[0]
            
            # Adaptive thresholds dá»±a trÃªn baseline - cáº£i thiá»‡n
            centroid_threshold = baseline['centroid_mean'] - baseline['centroid_std'] * 0.2  # Giáº£m threshold
            rolloff_threshold = baseline['rolloff_mean'] - baseline['rolloff_std'] * 0.2  # Giáº£m threshold
            
            # TÃ¬m frames cÃ³ spectral features phÃ¹ há»£p vá»›i voice
            voice_frames = []
            for i in range(len(spectral_centroids)):
                if (spectral_centroids[i] > centroid_threshold and 
                    spectral_rolloff[i] > rolloff_threshold):
                    voice_frames.append(i)
            
            # Chuyá»ƒn frames thÃ nh segments
            segments = self._frames_to_segments(voice_frames, sr)
            
            logger.info(f"ğŸ“Š Spectral detection: {len(segments)} segments")
            return segments
            
        except Exception as e:
            logger.warning(f"Spectral detection failed: {e}")
            return []
    
    def _detect_harmonic_pattern_improved(self, audio: np.ndarray, sr: int) -> List[Dict]:
        """PhÃ¡t hiá»‡n harmonic pattern cáº£i thiá»‡n"""
        try:
            # Harmonic-percussive separation
            harmonic, percussive = librosa.effects.hpss(audio)
            
            # TÃ­nh harmonic energy
            harmonic_rms = librosa.feature.rms(y=harmonic, frame_length=self.frame_length, hop_length=self.hop_length)[0]
            
            # Threshold cho harmonic energy
            harmonic_threshold = np.percentile(harmonic_rms, 25)  # Giáº£m threshold
            
            # TÃ¬m frames cÃ³ harmonic energy cao
            voice_frames = []
            for i, energy in enumerate(harmonic_rms):
                if energy > harmonic_threshold:
                    voice_frames.append(i)
            
            # Chuyá»ƒn frames thÃ nh segments
            segments = self._frames_to_segments(voice_frames, sr)
            
            logger.info(f"ğŸµ Harmonic detection: {len(segments)} segments")
            return segments
            
        except Exception as e:
            logger.warning(f"Harmonic detection failed: {e}")
            return []
    
    def _detect_voice_characteristics_improved(self, audio: np.ndarray, sr: int) -> List[Dict]:
        """PhÃ¡t hiá»‡n voice characteristics cáº£i thiá»‡n"""
        try:
            # MFCC features
            mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13, hop_length=self.hop_length)
            
            # Spectral contrast
            spectral_contrast = librosa.feature.spectral_contrast(y=audio, sr=sr, hop_length=self.hop_length)
            
            # Zero crossing rate
            zcr = librosa.feature.zero_crossing_rate(audio, frame_length=self.frame_length, hop_length=self.hop_length)[0]
            
            # Adaptive thresholds
            mfcc_threshold = np.percentile(mfccs[0], 20)  # Giáº£m threshold
            contrast_threshold = np.percentile(spectral_contrast[0], 20)  # Giáº£m threshold
            zcr_threshold = np.percentile(zcr, 20)  # Giáº£m threshold
            
            # TÃ¬m frames cÃ³ voice characteristics
            voice_frames = []
            for i in range(len(mfccs[0])):
                if (mfccs[0][i] > mfcc_threshold and 
                    spectral_contrast[0][i] > contrast_threshold and
                    zcr[i] > zcr_threshold):
                    voice_frames.append(i)
            
            # Chuyá»ƒn frames thÃ nh segments
            segments = self._frames_to_segments(voice_frames, sr)
            
            logger.info(f"ğŸ¤ Voice characteristics detection: {len(segments)} segments")
            return segments
            
        except Exception as e:
            logger.warning(f"Voice characteristics detection failed: {e}")
            return []
    
    def _smart_combine_segments_improved(self, energy_segments: List[Dict], spectral_segments: List[Dict], 
                                       harmonic_segments: List[Dict], voice_segments: List[Dict], 
                                       baseline: Dict) -> List[Dict]:
        """Káº¿t há»£p segments thÃ´ng minh - cáº£i thiá»‡n"""
        try:
            # Káº¿t há»£p táº¥t cáº£ segments
            all_segments = energy_segments + spectral_segments + harmonic_segments + voice_segments
            
            # Merge overlapping segments
            merged_segments = self._merge_overlapping_segments(all_segments)
            
            # Lá»c segments dá»±a trÃªn baseline - cáº£i thiá»‡n
            filtered_segments = []
            for segment in merged_segments:
                # Kiá»ƒm tra duration
                duration = segment['end'] - segment['start']
                if duration < 0.5:  # Bá» qua segments quÃ¡ ngáº¯n
                    continue
                
                # Kiá»ƒm tra vá»‹ trÃ­ - bá» qua segments á»Ÿ Ä‘áº§u file quÃ¡ sá»›m
                if segment['start'] < 1.0:  # Bá» qua segments á»Ÿ Ä‘áº§u file
                    continue
                
                # Kiá»ƒm tra confidence
                if segment['confidence'] < 0.3:  # Giáº£m threshold confidence
                    continue
                
                filtered_segments.append(segment)
            
            # Sáº¯p xáº¿p theo thá»i gian
            filtered_segments.sort(key=lambda x: x['start'])
            
            logger.info(f"ğŸ”— Smart combination: {len(filtered_segments)} segments after filtering")
            return filtered_segments
            
        except Exception as e:
            logger.warning(f"Smart combination failed: {e}")
            return []
    
    def _frames_to_segments(self, voice_frames: List[int], sr: int) -> List[Dict]:
        """Chuyá»ƒn frames thÃ nh segments"""
        if not voice_frames:
            return []
        
        segments = []
        current_start_frame = voice_frames[0]
        current_end_frame = voice_frames[0]
        
        for i in range(1, len(voice_frames)):
            if voice_frames[i] == current_end_frame + 1:
                current_end_frame = voice_frames[i]
            else:
                segments.append({
                    'start': current_start_frame * self.hop_length / sr,
                    'end': (current_end_frame + 1) * self.hop_length / sr,
                    'confidence': 1.0,
                    'method': 'improved_smart_detection'
                })
                current_start_frame = voice_frames[i]
                current_end_frame = voice_frames[i]
        
        segments.append({
            'start': current_start_frame * self.hop_length / sr,
            'end': (current_end_frame + 1) * self.hop_length / sr,
            'confidence': 1.0,
            'method': 'improved_smart_detection'
        })
        
        return segments
    
    def _merge_overlapping_segments(self, segments: List[Dict]) -> List[Dict]:
        """Merge overlapping segments"""
        if not segments:
            return []
        
        # Sáº¯p xáº¿p theo start time
        segments.sort(key=lambda x: x['start'])
        
        merged = []
        current_segment = segments[0]
        
        for i in range(1, len(segments)):
            next_segment = segments[i]
            
            # Náº¿u segments overlap hoáº·c gáº§n nhau
            if next_segment['start'] - current_segment['end'] <= 0.5:
                current_segment['end'] = max(current_segment['end'], next_segment['end'])
                current_segment['confidence'] = max(current_segment['confidence'], next_segment['confidence'])
            else:
                merged.append(current_segment)
                current_segment = next_segment
        
        merged.append(current_segment)
        return merged
