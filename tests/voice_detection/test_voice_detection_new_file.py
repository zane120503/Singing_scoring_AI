#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Test Voice Detection v·ªõi file m·ªõi
"""

import sys
import os
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))

import logging
import numpy as np
import librosa
import matplotlib.pyplot as plt

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_voice_detection_new_file():
    """Test Voice Detection v·ªõi file m·ªõi"""
    try:
        from src.ai.advanced_voice_detector import AdvancedVoiceDetector
        
        # Test file m·ªõi
        test_file = "D:\\singing scoring AI\\Audio_separator_ui\\clean_song_output\\62b6c4008acb2bcafc_mdx\\input.wav"
        
        if not os.path.exists(test_file):
            logger.error(f"Test file kh√¥ng t·ªìn t·∫°i: {test_file}")
            return False
        
        logger.info(f"üé§ Testing Voice Detection v·ªõi file m·ªõi...")
        logger.info(f"File: {test_file}")
        
        # Load audio ƒë·ªÉ ph√¢n t√≠ch
        audio, sr = librosa.load(test_file, sr=22050)
        duration = len(audio) / sr
        
        logger.info(f"Audio info: {duration:.2f}s, {sr} Hz, {len(audio)} samples")
        
        # Kh·ªüi t·∫°o detector
        detector = AdvancedVoiceDetector(sr)
        
        # Test c√°c ph∆∞∆°ng ph√°p kh√°c nhau
        methods = ["webrtc", "auto", "fallback"]
        
        for method in methods:
            logger.info(f"\n--- Testing method: {method} ---")
            segments = detector.detect_voice_activity_advanced(test_file, method=method)
            logger.info(f"Ph√°t hi·ªán {len(segments)} voice segments")
            
            if segments:
                for i, segment in enumerate(segments[:5]):  # Hi·ªÉn th·ªã 5 ƒëo·∫°n ƒë·∫ßu
                    logger.info(f"  Segment {i+1}: {segment['start']:.2f}s - {segment['end']:.2f}s (conf: {segment['confidence']:.2f}, method: {segment['method']})")
            else:
                logger.warning("  Kh√¥ng ph√°t hi·ªán voice segments")
        
        # T√¨m ƒëo·∫°n voice ƒë·∫ßu ti√™n ph√π h·ª£p
        first_voice = detector.find_first_voice_segment(test_file, min_duration=1.0)
        if first_voice['start'] > 0:
            logger.info(f"\nüéØ First suitable voice segment: {first_voice['start']:.2f}s - {first_voice['end']:.2f}s")
        else:
            logger.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y voice segment ph√π h·ª£p")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói trong test: {e}")
        import traceback
        traceback.print_exc()
        return False

def analyze_audio_manually_new_file():
    """Ph√¢n t√≠ch audio th·ªß c√¥ng ƒë·ªÉ hi·ªÉu t·∫°i sao kh√¥ng ph√°t hi·ªán voice"""
    try:
        test_file = "D:\\singing scoring AI\\Audio_separator_ui\\clean_song_output\\62b6c4008acb2bcafc_mdx\\input.wav"
        
        if not os.path.exists(test_file):
            logger.error(f"Test file kh√¥ng t·ªìn t·∫°i: {test_file}")
            return False
        
        logger.info(f"üîç Ph√¢n t√≠ch audio th·ªß c√¥ng...")
        
        # Load audio
        audio, sr = librosa.load(test_file, sr=22050)
        
        # T√≠nh to√°n c√°c features
        rms = librosa.feature.rms(y=audio)[0]
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
        zcr = librosa.feature.zero_crossing_rate(audio)[0]
        
        # T√≠nh to√°n th·ªëng k√™
        rms_mean = np.mean(rms)
        rms_std = np.std(rms)
        rms_threshold = rms_mean + 0.5 * rms_std
        
        centroid_mean = np.mean(spectral_centroids)
        centroid_std = np.std(spectral_centroids)
        
        zcr_mean = np.mean(zcr)
        zcr_std = np.std(zcr)
        
        logger.info(f"üìä Audio Analysis:")
        logger.info(f"   RMS Energy: mean={rms_mean:.4f}, std={rms_std:.4f}, threshold={rms_threshold:.4f}")
        logger.info(f"   Spectral Centroid: mean={centroid_mean:.1f} Hz, std={centroid_std:.1f} Hz")
        logger.info(f"   Zero Crossing Rate: mean={zcr_mean:.4f}, std={zcr_std:.4f}")
        
        # T√¨m c√°c frame c√≥ energy cao
        high_energy_frames = np.where(rms > rms_threshold)[0]
        if len(high_energy_frames) > 0:
            logger.info(f"   High energy frames: {len(high_energy_frames)} frames")
            logger.info(f"   First high energy frame: {high_energy_frames[0]}")
            logger.info(f"   Last high energy frame: {high_energy_frames[-1]}")
        else:
            logger.warning("   Kh√¥ng t√¨m th·∫•y high energy frames")
        
        # Ph√¢n t√≠ch chi ti·∫øt h∆°n
        logger.info(f"\nüîç Detailed Analysis:")
        
        # T√¨m c√°c ƒëo·∫°n c√≥ energy cao li√™n t·ª•c
        energy_threshold = np.percentile(rms, 30)
        voice_frames = np.where(rms > energy_threshold)[0]
        
        if len(voice_frames) > 0:
            logger.info(f"   Voice frames (30th percentile): {len(voice_frames)} frames")
            
            # T√¨m c√°c ƒëo·∫°n li√™n t·ª•c
            segments = []
            current_start = voice_frames[0]
            current_end = voice_frames[0]
            
            for i in range(1, len(voice_frames)):
                if voice_frames[i] - voice_frames[i-1] <= 2:  # Li√™n t·ª•c
                    current_end = voice_frames[i]
                else:
                    segments.append((current_start, current_end))
                    current_start = voice_frames[i]
                    current_end = voice_frames[i]
            
            segments.append((current_start, current_end))
            
            logger.info(f"   Continuous voice segments: {len(segments)}")
            for i, (start, end) in enumerate(segments[:5]):
                start_time = start * 512 / sr  # hop_length = 512
                end_time = end * 512 / sr
                duration = end_time - start_time
                logger.info(f"     Segment {i+1}: {start_time:.2f}s - {end_time:.2f}s (duration: {duration:.2f}s)")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói ph√¢n t√≠ch audio: {e}")
        return False

def create_improved_voice_detector():
    """T·∫°o Voice Activity Detector c·∫£i thi·ªán cho file m·ªõi"""
    try:
        logger.info("üîß T·∫°o Voice Activity Detector c·∫£i thi·ªán...")
        
        improved_detector_code = '''
def _detect_voice_activity_improved_v2(self, audio: np.ndarray, sr: int) -> List[Dict]:
    """Ph√°t hi·ªán voice activity c·∫£i thi·ªán v2 - d√†nh cho file karaoke"""
    try:
        # 1. RMS Energy Analysis v·ªõi threshold th·∫•p h∆°n
        rms = librosa.feature.rms(y=audio, frame_length=2048, hop_length=512)[0]
        
        # S·ª≠ d·ª•ng percentile th·∫•p h∆°n ƒë·ªÉ ph√°t hi·ªán voice nh·∫π
        rms_threshold = np.percentile(rms, 15)  # Th·∫•p h∆°n n·ªØa
        
        # 2. Spectral Centroid Analysis
        spectral_centroids = librosa.feature.spectral_centroid(y=audio, sr=sr)[0]
        
        # Voice frequency range (100-3500 Hz) - m·ªü r·ªông h∆°n
        voice_low = 100
        voice_high = 3500
        
        # 3. Zero Crossing Rate Analysis
        zcr = librosa.feature.zero_crossing_rate(audio)[0]
        
        # Voice c√≥ ZCR trung b√¨nh
        zcr_low = np.percentile(zcr, 5)   # Th·∫•p h∆°n
        zcr_high = np.percentile(zcr, 95) # Cao h∆°n
        
        # 4. Spectral Rolloff Analysis
        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)[0]
        rolloff_threshold = np.percentile(spectral_rolloff, 20)  # Th·∫•p h∆°n
        
        # 5. MFCC Analysis (cho voice characteristics)
        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)[0]
        
        # 6. Spectral Bandwidth Analysis
        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)[0]
        bandwidth_threshold = np.percentile(spectral_bandwidth, 25)
        
        # Detect voice frames
        voice_frames = []
        for i in range(len(rms)):
            # ƒêi·ªÅu ki·ªán ph√°t hi·ªán voice - c·∫ßn √≠t nh·∫•t 2/6 ƒëi·ªÅu ki·ªán
            rms_ok = rms[i] > rms_threshold
            centroid_ok = voice_low < spectral_centroids[i] < voice_high
            zcr_ok = zcr_low < zcr[i] < zcr_high
            rolloff_ok = spectral_rolloff[i] > rolloff_threshold
            bandwidth_ok = spectral_bandwidth[i] > bandwidth_threshold
            
            # C·∫ßn √≠t nh·∫•t 2/5 ƒëi·ªÅu ki·ªán
            if sum([rms_ok, centroid_ok, zcr_ok, rolloff_ok, bandwidth_ok]) >= 2:
                voice_frames.append(i)
        
        return self._frames_to_segments(voice_frames, sr)
        
    except Exception as e:
        logger.warning(f"Improved voice detection v2 failed: {e}")
        return []
'''
        
        # Ghi v√†o file
        with open("D:\\singing scoring AI\\improved_voice_detector_v2.py", "w", encoding="utf-8") as f:
            f.write(improved_detector_code)
        
        logger.info("‚úÖ Improved Voice Detector v2 code created")
        return True
        
    except Exception as e:
        logger.error(f"‚ùå L·ªói t·∫°o improved detector v2: {e}")
        return False

if __name__ == "__main__":
    print("=== TEST VOICE DETECTION NEW FILE ===")
    
    # Test voice detection
    print("\n1. Testing Voice Detection...")
    detection_success = test_voice_detection_new_file()
    
    # Analyze audio manually
    print("\n2. Analyzing audio manually...")
    analysis_success = analyze_audio_manually_new_file()
    
    # Create improved detector
    print("\n3. Creating improved detector v2...")
    improved_success = create_improved_voice_detector()
    
    # K·∫øt qu·∫£
    print("\n=== KET QUA ===")
    print(f"Voice Detection: {'OK' if detection_success else 'ERROR'}")
    print(f"Audio Analysis: {'OK' if analysis_success else 'ERROR'}")
    print(f"Improved Detector v2: {'OK' if improved_success else 'ERROR'}")
    
    if detection_success and analysis_success:
        print("\nCAN CAI THIEN VOICE DETECTION CHO FILE MOI!")
    else:
        print("\nCO VAN DE VOI VOICE DETECTION!")
